{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python382jvsc74a57bd05edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9",
      "display_name": "Python 3.8.2 64-bit ('usr')"
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    },
    "colab": {
      "name": "GenderClassification(TopicControl).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgbEzz_8zOBu"
      },
      "source": [
        "# =============================================================================\n",
        "# Carico i file\n",
        "# =============================================================================\n",
        "train_file = r'data/training.txt'\n",
        "train_url=r'data/training1.csv'\n",
        "test_file = r'data/test.txt'\n",
        "test_url = r'data/test1.csv'\n",
        "delimiter = ','\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StH_AK8nzZjq"
      },
      "source": [
        "import csv\n",
        "x_train = list()\n",
        "y_train = list()\n",
        "topic_train = list()\n",
        "with open(train_url, encoding='utf-8', newline='') as infile:\n",
        "    reader = csv.reader(infile, delimiter=delimiter)\n",
        "    for row in reader:\n",
        "        x_train.append(row[5])\n",
        "        y_train.append(row[4])\n",
        "        topic_train.append(row[3])\n",
        "\n",
        "x_test = list()\n",
        "y_test = list()\n",
        "topic_test = list()\n",
        "with open(test_url, encoding='utf-8', newline='') as infile:\n",
        "    reader = csv.reader(infile, delimiter=delimiter)\n",
        "    for row in reader:\n",
        "        x_test.append(row[5])\n",
        "        y_test.append(row[4])\n",
        "        topic_test.append(row[3])\n",
        "        \n",
        "len(x_train),len(y_train),len(x_test),len(y_test)\n",
        "y_train.pop(0) # elimino il primo elemento che è la parola \"gender\"\n",
        "y_test.pop(0) # elimino il primo elemento che è la parola \"gender\"\n",
        "set(y_train)\n",
        "sample_idx = 10\n",
        "\n",
        "x_train.pop(0) # elimino il primo elemento che è la parola \"post\"\n",
        "x_test.pop(0) # elimino il primo elemento che è la parola \"post\"\n",
        "x_train[sample_idx]\n",
        "y_train[sample_idx]\n",
        "\n",
        "topic_train.pop(0) # elimino il primo elemento che è \"topic\"\n",
        "topic_test.pop(0) # elimino il primo elemento che è \"topic\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-qX9ha1zZmN"
      },
      "source": [
        "# =============================================================================\n",
        "# Campiono i dati per rimediare allo sbilanciamento di questi\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "df_train = pd.DataFrame(data = {\"x\": x_train, \"y\": y_train})\n",
        "#df_test = pd.DataFrame(data = {\"x\": x_test, \"y\": y_test})\n",
        "\n",
        "# Training Set\n",
        "df_train = df_train.groupby('y', group_keys=False).apply(lambda x: x.sample(min(len(x), 125)))\n",
        "df_train[\"y\"].tolist().count(\"M\")\n",
        "\n",
        "x_train = df_train[\"x\"].tolist()\n",
        "y_train = df_train[\"y\"].tolist()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sHt7VbdzZoa"
      },
      "source": [
        "# =============================================================================\n",
        "# Seleziono il topic       commentare questa parte per prendere tutti topic\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "topics = df[\"topic\"].unique()\n",
        "print(topics)\n",
        "from collections import Counter\n",
        "Counter(df[\"topic\"])\n",
        "# Seleziono il training set\n",
        "indices = [i for i, x in enumerate(topic_train) if x == \"CELEBRITIES\"]\n",
        "x_train = [x_train[i] for i in indices]\n",
        "y_train = [y_train[i] for i in indices]\n",
        "# len(x_train), len(y_train)\n",
        "print(\"Lunghezza del training set: \", len(x_train))\n",
        "print(\"Percentuale di maschi nel training set: \", round(y_train.count(\"M\") / len(y_train), 3))\n",
        "\n",
        "# Seleziono il test set\n",
        "indices = [i for i, x in enumerate(topic_test) if x == \"CELEBRITIES\"]\n",
        "x_test = [x_test[i] for i in indices]\n",
        "y_test = [y_test[i] for i in indices]\n",
        "# len(x_test), len(y_test)\n",
        "print(\"Lunghezza del test set: \", len(x_test))\n",
        "print(\"Percentuale di maschi nel test set: \", round(y_test.count(\"M\") / len(y_test), 3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA7eOxVyzZqo"
      },
      "source": [
        "# =============================================================================\n",
        "# Operazioni sul testo\n",
        "# =============================================================================\n",
        "# Rendo minuscola la prima parola dopo il punto\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Elimino la parola \"post\"\n",
        "regex = re.compile(\"\\\\bpost\\\\b\")\n",
        "for i in range(0, len(x_train)):\n",
        "    x_train[i] = regex.sub('', x_train[i])\n",
        "for i in range(0, len(x_test)):\n",
        "    x_test[i] = regex.sub('', x_test[i])\n",
        "# Elimino la punteggiatura\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "for i in range(0, len(x_train)):\n",
        "    x_train[i] = regex.sub('', x_train[i])\n",
        "for i in range(0, len(x_test)):\n",
        "    x_test[i] = regex.sub('', x_test[i])\n",
        "# Elimino i numeri\n",
        "regex = re.compile(\"[0-9]+\")\n",
        "for i in range(0, len(x_train)):\n",
        "    x_train[i] = regex.sub('', x_train[i])\n",
        "for i in range(0, len(x_test)):\n",
        "    x_test[i] = regex.sub('', x_test[i])\n",
        "\n",
        "### EXTRA: NON IMPORTANTE\n",
        "# Rendo minuscola la prima parola dopo il punto\n",
        "# for i in range(0, len(x_train)):\n",
        "    # x_train[i] = re.sub('(?<=\\.\\s)(\\w+)', lambda m: m.group().lower(), x_train[i])\n",
        "# for i in range(0, len(x_test)):\n",
        "    # x_test[i] = re.sub('(?<=\\.\\s)(\\w+)', lambda m: m.group().lower(), x_test[i])\n",
        "# Rendo minuscola la prima parola\n",
        "# for i in range(0, len(x_train)):\n",
        "    # x_train[i] = x_train[i][1].lower() + x_train[i][2:] # il primo carattere di ogni post è \\n, quindi parto da 1, non da 0\n",
        "# for i in range(0, len(x_test)):\n",
        "    # x_test[i] = x_test[i][1].lower() + x_test[i][2:] # il primo carattere di ogni post è \\n, quindi parto da 1, non da 0\n",
        "###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT8Qe_WUzZs7"
      },
      "source": [
        "# =============================================================================\n",
        "# Funzioni\n",
        "# =============================================================================\n",
        "import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "stopword_list = stopwords.words('italian')\n",
        "\n",
        "from collections import defaultdict\n",
        "tag_map = defaultdict(lambda : wordnet.NOUN)\n",
        "tag_map['J'] = wordnet.ADJ\n",
        "tag_map['V'] = wordnet.VERB\n",
        "tag_map['R'] = wordnet.ADV\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "doc_counter = 0\n",
        "def reset_counter():\n",
        "    global doc_counter\n",
        "    doc_counter = 0\n",
        "\n",
        "def increase_counter():\n",
        "    global doc_counter\n",
        "    doc_counter += 1\n",
        "    if doc_counter % 100 == 0:\n",
        "        print(doc_counter)\n",
        "\n",
        "def nltk_ngram_tokenizer(text):\n",
        "    increase_counter()\n",
        "\n",
        "    # tokens, skipping stopwords\n",
        "    tokens = [token for token in word_tokenize(text) if token not in stopword_list]\n",
        "\n",
        "    # we use a simple nltk function to create ngrams\n",
        "    bigrams = ['BI_'+w1+'_'+w2 for w1,w2 in nltk.ngrams(tokens,2)]\n",
        "    trigrams = ['TRI_'+p1+'_'+p2+'_'+p3 for p1,p2,p3 in nltk.ngrams(tokens,3)]\n",
        "\n",
        "    all_tokens = list()\n",
        "    all_tokens.extend(tokens)\n",
        "    all_tokens.extend(bigrams)\n",
        "    all_tokens.extend(trigrams)\n",
        "    return all_tokens\n",
        "\n",
        "def nltk_nlp_tokenizer(text):\n",
        "    increase_counter()\n",
        "\n",
        "    # tokens, skipping stopwords\n",
        "    tokens = [token for token in word_tokenize(text) if token not in stopword_list]\n",
        "\n",
        "    # lemmatized tokens\n",
        "    lemmas = list()\n",
        "    for token, tag in pos_tag(tokens):\n",
        "  \t    lemmas.append('LEMMA_'+lemmatizer.lemmatize(token, tag_map[tag[0]]))\n",
        "\n",
        "    # we use a simple nltk function to create ngrams\n",
        "    lemma_bigrams = ['BI_'+p1+'_'+p2 for p1,p2 in nltk.ngrams(lemmas,2)]\n",
        "    lemma_trigrams = ['TRI_'+p1+'_'+p2+'_'+p3 for p1,p2,p3 in nltk.ngrams(lemmas,3)]\n",
        "\n",
        "    all_tokens = list()\n",
        "    all_tokens.extend(lemmas)\n",
        "    all_tokens.extend(lemma_bigrams)\n",
        "    all_tokens.extend(lemma_trigrams)\n",
        "    return all_tokens\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ut7cRzjzZvM"
      },
      "source": [
        "# Rendo la risposta binaria\n",
        "import numpy as np\n",
        "# y_train[sample_idx] è una delle due classi, nel nostro caso M. Quando vado a fare y_train == y_train[sample_idx], in pratica\n",
        "# metto TRUE per i maschi (M) e FALSE per le femmine (F).\n",
        "y_train_bin = np.asarray(y_train)==y_train[sample_idx]\n",
        "y_test_bin = np.asarray(y_test)==y_train[sample_idx]\n",
        "y_train_bin,y_test_bin\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "vect = CountVectorizer(analyzer = nltk_nlp_tokenizer, min_df=5) # Passiamo la funzione spacy_nlp_tokenizer per ottenere sia parole singole che n-grams\n",
        "reset_counter()\n",
        "X_train_tok = vect.fit_transform(x_train)\n",
        "reset_counter()\n",
        "X_test_tok = vect.transform(x_test)\n",
        "\n",
        "len(vect.vocabulary_)\n",
        "X_train_tok[:5]\n",
        "print(X_train_tok[:5])\n",
        "vect.inverse_transform(X_train_tok[:5])\n",
        "for feat,freq in zip(vect.inverse_transform(X_train_tok[:5])[1],X_train_tok[:5].data):\n",
        "  print(feat,freq)\n",
        "\n",
        "# FEATURE SELECTION\n",
        "bin_sel = SelectKBest(chi2, k=150)\n",
        "bin_sel.fit(X_train_tok,y_train_bin)\n",
        "X_train_sel_bin = bin_sel.transform(X_train_tok)\n",
        "X_test_sel_bin = bin_sel.transform(X_test_tok)\n",
        "\n",
        "bin_sel.get_support()\n",
        "X_train_sel_bin\n",
        "print(\"Features\")\n",
        "print(X_train_sel_bin[:5])\n",
        "print(vect.inverse_transform(bin_sel.inverse_transform(X_train_sel_bin[:5])))\n",
        "\n",
        "# PESI CON TF-IDF\n",
        "tfidf = TfidfTransformer()\n",
        "tfidf.fit(X_train_sel_bin)\n",
        "X_train_vec_bin = tfidf.transform(X_train_sel_bin)\n",
        "X_test_vec_bin =tfidf.transform(X_test_sel_bin)\n",
        "print(\"Pesi Tf-IdF\")\n",
        "print(X_train_vec_bin[:5])\n",
        "for feat,weight,freq in zip(vect.inverse_transform(bin_sel.inverse_transform(X_train_vec_bin[:5]))[1],X_train_vec_bin[:5].data,X_train_sel_bin[:5].data):\n",
        "  print(feat,weight,freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1RqltmdzZxZ"
      },
      "source": [
        "# =============================================================================\n",
        "# Support Vector Machine ma con la Pipeline\n",
        "# =============================================================================\n",
        "bin_pipeline = Pipeline([\n",
        "    ('sel', SelectKBest(chi2, k=150)),  # feature selection\n",
        "    ('tfidf', TfidfTransformer()),  # weighting\n",
        "    ('learner', LinearSVC())  # learning algorithm\n",
        "])\n",
        "\n",
        "reset_counter()\n",
        "bin_pipeline.fit(X_train_tok,y_train_bin)\n",
        "\n",
        "# ACCURACY\n",
        "reset_counter()\n",
        "bin_predictions = bin_pipeline.predict(X_test_tok)\n",
        "correct = 0\n",
        "for prediction,true_label in zip(bin_predictions, y_test_bin):\n",
        "    if prediction==true_label:\n",
        "        correct += 1\n",
        "print(\"Accuracy:\", correct/len(bin_predictions))\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print('Classification report del LinearSVC con la pipeline:')\n",
        "print(classification_report(y_test_bin, bin_predictions))\n",
        "print('Confusion matrix:')\n",
        "cm = confusion_matrix(y_test_bin, bin_predictions)\n",
        "print(cm)\n",
        "\n",
        "tokenizer = vect\n",
        "selector = bin_pipeline.named_steps['sel']\n",
        "classifier = bin_pipeline.named_steps['learner']\n",
        "\n",
        "feature_names = tokenizer.get_feature_names()\n",
        "feats_w_score = list()\n",
        "for index,(selected,score) in enumerate(zip(selector.get_support(),selector.scores_)):\n",
        "    feats_w_score.append((score,selected,feature_names[index]))\n",
        "feats_w_score = sorted(feats_w_score)\n",
        "len(feats_w_score)\n",
        "\n",
        "feats_w_score[:100],feats_w_score[-100:]\n",
        "feats_w_classifier_weight = list()\n",
        "for index,weight in enumerate(selector.inverse_transform(classifier.coef_)[0]):\n",
        "    if weight!=0:\n",
        "        feats_w_classifier_weight.append((weight,feature_names[index]))\n",
        "feats_w_classifier_weight = sorted(feats_w_classifier_weight)\n",
        "len(feats_w_classifier_weight)\n",
        "\n",
        "feats_w_classifier_weight[-100:]\n",
        "feats_w_classifier_weight[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1zP5XzPzZzZ"
      },
      "source": [
        "# =============================================================================\n",
        "# Decision Tree\n",
        "# =============================================================================\n",
        "dt_bin_pipeline = Pipeline([\n",
        "    ('sel', SelectKBest(chi2, k=150)),  # feature selection\n",
        "    ('tfidf', TfidfTransformer()),  # weighting\n",
        "    ('learner', DecisionTreeClassifier())  # learning algorithm\n",
        "])\n",
        "\n",
        "dt_bin_pipeline.fit(X_train_tok,y_train_bin)\n",
        "bin_predictions = dt_bin_pipeline.predict(X_test_tok)\n",
        "\n",
        "print('Classification report del Decision Tree:')\n",
        "print(classification_report(y_test_bin, bin_predictions))\n",
        "print('Confusion matrix:')\n",
        "cm = confusion_matrix(y_test_bin, bin_predictions)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbgCD2MRzZ1t"
      },
      "source": [
        "# =============================================================================\n",
        "# Multinomial NB\n",
        "# =============================================================================\n",
        "nb_bin_pipeline = Pipeline([\n",
        "    ('sel', SelectKBest(chi2, k=150)),  # feature selection\n",
        "    ('learner', MultinomialNB())  # learning algorithm\n",
        "])\n",
        "\n",
        "nb_bin_pipeline.fit(X_train_tok,y_train_bin)\n",
        "bin_predictions = nb_bin_pipeline.predict(X_test_tok)\n",
        "\n",
        "print('Classification report del MultinomialNB:')\n",
        "print(classification_report(y_test_bin, bin_predictions))\n",
        "print('Confusion matrix:')\n",
        "cm = confusion_matrix(y_test_bin, bin_predictions)\n",
        "print(cm)\n",
        "\n",
        "\n",
        "tokenizer = vect\n",
        "selector = nb_bin_pipeline.named_steps['sel']\n",
        "classifier = nb_bin_pipeline.named_steps['learner']\n",
        "classifier.class_log_prior_,classifier.feature_log_prob_, len(classifier.feature_log_prob_[0])\n",
        "ratio = classifier.feature_log_prob_[0]/classifier.feature_log_prob_[1]\n",
        "\n",
        "feats_w_classifier_weight = list()\n",
        "feature_names = tokenizer.get_feature_names()\n",
        "for index,weight in enumerate(selector.inverse_transform([ratio])[0]):\n",
        "    if weight!=0:\n",
        "        feats_w_classifier_weight.append((weight,feature_names[index]))\n",
        "feats_w_classifier_weight = sorted(feats_w_classifier_weight)\n",
        "len(feats_w_classifier_weight)\n",
        "\n",
        "feats_w_classifier_weight[-100::-1]\n",
        "feats_w_classifier_weight[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qciE4ny6zZ4K"
      },
      "source": [
        "# =============================================================================\n",
        "# Logistic Regression \n",
        "# =============================================================================\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('sel', SelectKBest(chi2, k=150)),  # feature selection\n",
        "    ('tfidf', TfidfTransformer()),  # weighting\n",
        "    ('learner', LogisticRegression(solver=\"liblinear\",multi_class=\"auto\"))  # learning algorithm\n",
        "])\n",
        "\n",
        "classifier = pipeline.fit(X_train_tok,y_train_bin)\n",
        "predictions = classifier.predict(X_test_tok)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print('Classification report della Logistic Regression:')\n",
        "print(classification_report(y_test_bin, bin_predictions))\n",
        "print('Confusion matrix:')\n",
        "cm = confusion_matrix(y_test_bin, bin_predictions)\n",
        "print(cm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7UmajBZzOB1"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "   \n",
        "\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    # Plot n_samples vs fit_times\n",
        "    axes[1].grid()\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
        "    axes[1].set_xlabel(\"Training examples\")\n",
        "    axes[1].set_ylabel(\"fit_times\")\n",
        "    axes[1].set_title(\"Scalability of the model\")\n",
        "\n",
        "    # Plot fit_time vs score\n",
        "    axes[2].grid()\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
        "    axes[2].set_xlabel(\"fit_times\")\n",
        "    axes[2].set_ylabel(\"Score\")\n",
        "    axes[2].set_title(\"Performance of the model\")\n",
        "\n",
        "    return plt\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
        "\n",
        "X, y = load_digits(return_X_y=True)\n",
        "\n",
        "title = \"Learning Curves (MultinomialNB)\"\n",
        "# Cross validation with 100 iterations to get smoother mean test and train\n",
        "# score curves, each time with 20% data randomly selected as a validation set.\n",
        "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
        "\n",
        "estimator = MultinomialNB()\n",
        "plot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
        "                    cv=cv, n_jobs=4)\n",
        "\n",
        "title = r\"Learning Curves (LinearSVM)\"\n",
        "# SVC is more expensive so we do a lower number of CV iterations:\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "estimator = LinearSVC()\n",
        "plot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n",
        "                    cv=cv, n_jobs=4)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSCxWWoozOB2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
